{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89q7mbPv9NjU"
   },
   "source": [
    "# Prognostics for Turbofine Engines with 1D Convolutional Neural Networks ü©∫\n",
    "\n",
    "Prognostics is the prediction of Remaining useful life of instance of failure of a component based on the knowledge about current and future coditions of operation (obtained through various sensors or physical models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFd6spkDEvi4"
   },
   "source": [
    "The new C-MAPSS dataset DS02 from NASA provides degradation trajectories of 9 turbofan engines with unknown and different initial health condition for complete flights and two failure modes (HPT efficiency degradation & HPT efficiency degradation combined with LPT efficiency and capacity degradation). The data were synthetically generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model. The data contains multivariate sensors readings of the complete run-to-failure trajectories. Therefore, the records stop at the cycle/time the engine failed. A total number of 6.5M time stamps are available. Dataset copyright (c) by Manuel Arias.\n",
    "\n",
    "**For training simplicity, the dataset has been preprocessed. The dataset has been downsampled from 1Hz to 0.1 Hz with an IIR 8th Order Chebyshev filter. Data format has been converted from double to float precison.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3QI5TCpSmU0"
   },
   "source": [
    "## Imports üíº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CVNWlBJ9Ija"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdxpcrt_SqI4"
   },
   "source": [
    "## Download dataset üîó\n",
    "\n",
    "Download the dataset from [Google Drive Link](https://drive.google.com/file/d/1jXC3BQmEkupXkJbuXM52-Lf6ADLzR4Rr/view?usp=sharing) and put it in the folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ccLNkIa9Xvx",
    "outputId": "1dd1e3d4-4587-46f1-871e-f72159be57c3"
   },
   "outputs": [],
   "source": [
    "folder = os.getcwd()\n",
    "filename = f'{folder}/ncmapps_ds02.csv'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3y1SgKPqSwq"
   },
   "source": [
    "# Data exploration üîç\n",
    "\n",
    "<img src=\"images/cmapss.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0pa75ICAUU1"
   },
   "source": [
    "### Feature descriptions\n",
    "\n",
    "| Symbol |           Description           | Units |\n",
    "|:------:|:-------------------------------:|:-----:|\n",
    "|   Wf   |            Fuel flow            |  pps  |\n",
    "|   Nf   |        Physical fan speed       |  rpm  |\n",
    "|   Nc   |       Physical core speed       |  rpm  |\n",
    "|   T24  | Total temperature at LPC outlet |   ¬∞R  |\n",
    "|   T30  | Total temperature at HPC outlet |   ¬∞R  |\n",
    "|   T48  | Total temperature at HPT outlet |   ¬∞R  |\n",
    "|   T50  | Total temperature at LPT outlet |   ¬∞R  |\n",
    "|   P15  |  Total pressure in bypass-duct  |  psia |\n",
    "|   P2   |   Total pressure at fan inlet   |  psia |\n",
    "|   P21  |   Total pressure at fan outlet  |  psia |\n",
    "|   P24  |   Total pressure at LPC outlet  |  psia |\n",
    "|  Ps30  |  Static pressure at HPC outlet  |  psia |\n",
    "|   P40  | Total pressure at burner outlet |  psia |\n",
    "|   P50  |   Total pressure at LPT outlet  |  psia |\n",
    "|   alt  |             Altitude            |   ft  |\n",
    "|  Mach  |        Flight Mach number       |   -   |\n",
    "|   TRA  |     Throttle‚Äìresolver angle     |   %   |\n",
    "|   T2   |  Total temperature at fan inlet |   ¬∞R  |\n",
    "|  cycle |       Flight cycle number       |   -   |\n",
    "|   Fc   |           Flight class          |   -   |\n",
    "|   hs   |           Health state          |   -   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "Au1CJA3b95-f",
    "outputId": "b873d351-3e30-4240-d04e-c24c4ffc9520"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "0pxG0gHm_Jc9",
    "outputId": "0cdca513-b6e5-403c-91a0-8c99c83d8159"
   },
   "outputs": [],
   "source": [
    "df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots are usefull to see the data distribution and the outliers\n",
    "df_columns_name = df.columns\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "print(numeric_columns)\n",
    "#df.boxplot(column =numeric_columns, grid = False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "df.boxplot(numeric_columns[0] , ax=ax[0])\n",
    "df.boxplot(numeric_columns[1:4], ax=ax[1])\n",
    "df.boxplot(numeric_columns[4:8], ax=ax[2])\n",
    "plt.suptitle('Test')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 5))\n",
    "df.boxplot(numeric_columns[8] , ax=ax[0])\n",
    "df.boxplot(numeric_columns[9], ax=ax[1])\n",
    "df.boxplot(numeric_columns[10], ax=ax[2])\n",
    "df.boxplot(numeric_columns[11], ax=ax[3])\n",
    "plt.suptitle('Test 2')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 5))\n",
    "df.boxplot(numeric_columns[12] , ax=ax[0])\n",
    "df.boxplot(numeric_columns[13], ax=ax[1])\n",
    "df.boxplot(numeric_columns[14], ax=ax[2])\n",
    "df.boxplot(numeric_columns[15], ax=ax[3])\n",
    "plt.suptitle('Test 3')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 5))\n",
    "df.boxplot(numeric_columns[16] , ax=ax[0])\n",
    "df.boxplot(numeric_columns[17] , ax=ax[1])\n",
    "df.boxplot(numeric_columns[18], ax=ax[2])\n",
    "df.boxplot(numeric_columns[19], ax=ax[3])\n",
    "plt.suptitle('Test 4')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "df.boxplot(numeric_columns[20], ax=ax[0])\n",
    "df.boxplot(numeric_columns[21], ax=ax[1])\n",
    "df.boxplot(numeric_columns[22], ax=ax[2])\n",
    "plt.suptitle('Test 5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5hBbUbqCIMB"
   },
   "source": [
    "### Flight Classes\n",
    "\n",
    "The units are divided into three flight classes depending on whether the unit is operating short-length flights (i.e., flight class 1), medium-length flights (i.e., flight class 2), or long-length flights (i.e., flight class 2). A number of real flight conditions are available within each of the flight classes.\n",
    "\n",
    "| Flight Class   | Flight Length [h]\n",
    "| :-----------:  | :-----------:    \n",
    "| 1              |    1 to 3        \n",
    "| 2              |    3 to 5        \n",
    "| 3              |    5 to 7        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "BY-bP6gMoLSn",
    "outputId": "c434b014-d132-489b-ffc0-0f74f5c413a5"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(\n",
    "    subset=['unit','Fc'], keep='last'\n",
    "    ).plot(\n",
    "        x='unit', y='Fc', \n",
    "        kind='bar', \n",
    "        xlabel='Unit # [-]', \n",
    "        ylabel='Flight Class # [-]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVxR_2YcsAun"
   },
   "source": [
    "## Feature Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-WToY0EQ8gJ"
   },
   "outputs": [],
   "source": [
    "LABELS = ['RUL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQUvTSv8GSeE"
   },
   "source": [
    "Operative Conditions ($w$)\n",
    "\n",
    "DASHlink- Flight Data For Tail 687.(2012). Retrieved on 2019-01-29 from https://c3.nasa.gov/dashlink/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrHh3rYqGhGt"
   },
   "outputs": [],
   "source": [
    "W_VAR = ['alt', 'Mach', 'TRA', 'T2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQvRwIJqGN7H"
   },
   "source": [
    "Sensor readings ($X_s$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjgyPJPNGiOI"
   },
   "outputs": [],
   "source": [
    "XS_VAR = ['T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "k_7o8zlKGmWq",
    "outputId": "b2b28b40-09ad-4249-cb13-b6bad0ca5978"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "sns.heatmap(df[W_VAR+XS_VAR+LABELS].corr(), cmap=\"vlag\")\n",
    "# Adding the LABELS to the heat map allows us to see which sensor is most correlated.\n",
    "# The T48 and T50 sensors are the most correlated (monotonically) with RUL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJIwiz_LFDHM"
   },
   "source": [
    "### Flight Traces\n",
    "visualize a single flight trace of a given unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "42WQNKPuo5On",
    "outputId": "41deb687-571c-499a-cfe8-44b4757ec517"
   },
   "outputs": [],
   "source": [
    "# We are trying to compare a flight during the first cycles measured and a flight during the last cycles with a similar altitude and speed curve for the same unit.\n",
    "# Seein isn't easy to find\n",
    "unit = 18\n",
    "cycle = 3\n",
    "df_u_sel = df.loc[(df.unit == unit) & (df.cycle == cycle)][W_VAR+XS_VAR+LABELS]# df.loc[(df.unit == unit) & (df.cycle == cycle)][W_VAR+XS_VAR]\n",
    "df_u_sel.reset_index(inplace=True, drop=True)\n",
    "axes = df_u_sel.plot(figsize=(12, 10), subplots=True, layout=(5, 4))\n",
    "\n",
    "unit = 18\n",
    "cycle = 70\n",
    "df_u_sel = df.loc[(df.unit == unit) & (df.cycle == cycle)][W_VAR+XS_VAR+LABELS]# df.loc[(df.unit == unit) & (df.cycle == cycle)][W_VAR+XS_VAR]\n",
    "df_u_sel.reset_index(inplace=True, drop=True)\n",
    "axes = df_u_sel.plot(figsize=(12, 10), subplots=True, layout=(5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AYnARDgvcir"
   },
   "source": [
    "### Flight envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "F8YH_UgKvl-B",
    "outputId": "fbe278b5-5bc6-468a-9f50-92be11b2b150"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "for i in [3, 2, 1]:\n",
    "    df.loc[df['Fc'] == i].plot(x='Mach', y='alt', alpha=0.8, label=f'Fc={i}', ax=ax, lw=5)\n",
    "plt.xlabel('Mach Number - [-]')\n",
    "plt.ylabel('Flight Altitude - [ft]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFMiKdPj2Tam"
   },
   "source": [
    "## Histogram of Flight Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "KjkHl4fNyrMz",
    "outputId": "45936d24-c78b-4599-b72b-e0d442f23763"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, var in enumerate(W_VAR):\n",
    "    sns.kdeplot(data=df[W_VAR+['unit']], x=var, hue='unit',shade = True, gridsize=100, ax=axes[i//2][i%2], palette=sns.color_palette(\"husl\", 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impact of cycle\n",
    "\n",
    "filtered_unit = 5\n",
    "filtered_df = df[df['unit'] == filtered_unit]\n",
    "\n",
    "# Get the minimum and maximum values of 'cycle' within the first filter\n",
    "min_cycle = filtered_df['cycle'].min()\n",
    "max_cycle = filtered_df['cycle'].max()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop through each cycle in the filtered range and plot only every 10th cycle\n",
    "for i in range(min_cycle, max_cycle + 1):\n",
    "    if i % 10 == 0:  # Only plot every 10th cycle\n",
    "        # Filter data for the current cycle\n",
    "        cycle_data = filtered_df[filtered_df['cycle'] == i]\n",
    "        \n",
    "        # Define x-axis as the length of the cycle (0 to length of cycle_data - 1)\n",
    "        x_values = range(len(cycle_data))\n",
    "        y_values = cycle_data['T50']\n",
    "        \n",
    "        # Plot the curve for the current cycle\n",
    "        plt.plot(x_values, y_values, label=f'Cycle {i}')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('number of measure')\n",
    "plt.ylabel('T50')\n",
    "plt.title(f'T50 evolution every 10th Cycle for the unit {filtered_unit}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFJVdOWqqekG"
   },
   "source": [
    "# Developing your first prognostics model üíª\n",
    "Step by step, you will learn how to build your first prognostics model from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNBXLCmgFImE"
   },
   "source": [
    "## Define Sequence Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVFFWkhwFHP3"
   },
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, dataframe, window=50, stride=1, horizon=1, device='cpu'):\n",
    "        \"\"\"Sliding window dataset with RUL label\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): dataframe containing scenario descriptors and sensor reading\n",
    "            window (int, optional): sequence window length. Defaults to 50.\n",
    "            stride (int, optional): data stride length. Defaults to 1.\n",
    "            horizon (int, optional): prediction forcasting length. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.stride = stride\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        self.X = np.array(dataframe[XS_VAR+W_VAR].values).astype(np.float32)\n",
    "        self.y = np.array(dataframe['RUL'].values).astype(np.float32)\n",
    "        if 'ds' in dataframe.columns:\n",
    "            unqiue_cycles = dataframe[['ds', 'unit', 'cycle']].value_counts(sort=False)\n",
    "        else:\n",
    "            unqiue_cycles = dataframe[['unit', 'cycle']].value_counts(sort=False)\n",
    "        self.indices = torch.from_numpy(self._get_indices(unqiue_cycles)).to(device)\n",
    "\n",
    "    # TODO add comment\n",
    "    def _get_indices(self, unqiue_cycles):\n",
    "        '''\n",
    "        unqiue_cycles (pd.DataFrame): dataframe containing the values in columns 'ds'(if exist) , 'unit' , 'cycle' \n",
    "        and their occurance (the order of the datais preserved)\n",
    "\n",
    "        This function return the indexes of each (start_recording_flight, end_recording_flight) pair in the overall dataframe\n",
    "        Based on the 'unit' : motor ID and the 'cycle' : flight number\n",
    "        Plus the 'ds' : ... (if exist)\n",
    "        '''\n",
    "        cycles = unqiue_cycles.to_numpy() \n",
    "        idx_list = []\n",
    "        for i, c_count in enumerate(cycles):\n",
    "            c_start = sum(cycles[:i]) \n",
    "            c_end = c_start + (c_count - self.window - self.horizon)\n",
    "            if c_end + self.horizon < len(self.X): # handling y not in the last seq case\n",
    "                idx_list += [_ for _ in np.arange(c_start, c_end + 1, self.stride)] \n",
    "        return np.asarray([(idx, idx+self.window) for idx in idx_list])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        i_start, i_stop = self.indices[i]\n",
    "        x = self.X[i_start:i_stop, :]\n",
    "        y = self.y[i_start]\n",
    "        x = x.permute(1, 0)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8_R6aEIG2Iy"
   },
   "outputs": [],
   "source": [
    "def create_datasets(df, window_size, train_units, test_units, device='cpu'):\n",
    "    df_train = df[df['unit'].isin(train_units)]\n",
    "    train_dataset = SlidingWindowDataset(df_train, window=window_size)\n",
    "\n",
    "    df_test = df[df['unit'].isin(test_units)]    \n",
    "    test_dataset = SlidingWindowDataset(df_test, window=window_size)\n",
    "\n",
    "    # normalizing features\n",
    "    scaler = MinMaxScaler()\n",
    "    train_dataset.X = scaler.fit_transform(train_dataset.X)\n",
    "    test_dataset.X = scaler.transform(test_dataset.X)\n",
    "\n",
    "    # convert numpy array to tensors\n",
    "    datasets = [train_dataset, test_dataset]\n",
    "    for d in datasets:\n",
    "        d.X = torch.from_numpy(d.X).to(device)\n",
    "        d.y = torch.from_numpy(d.y).to(device)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def create_data_loaders(datasets, batch_size=256, val_split=0.2):\n",
    "    # fixed seed for data splits for reproducibility\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    d_train, d_test = datasets\n",
    "    dataset_size = len(d_train)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_split * dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(d_train, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(d_train, batch_size=batch_size, sampler=valid_sampler)\n",
    "    test_loader = DataLoader(d_test, batch_size=batch_size, shuffle=False)      \n",
    "\n",
    "    d_info = f\"train_size: {len(train_indices)}\\t\"\n",
    "    d_info += f\"validation_size: {len(val_indices)}\\t\"\n",
    "    d_info += f\"test_size: {len(d_test)}\"\n",
    "    print(d_info)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iuOuWQYuHlw"
   },
   "source": [
    "## Define Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ6S9PhqOA_G"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        n_epochs=20,\n",
    "        criterion=nn.MSELoss(),\n",
    "        model_name='best_model',\n",
    "        seed=42,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        self.seed = seed\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        # adding time_stamp to model name to make sure the save models don't overwrite each other, \n",
    "        # you can customize your own model name with hyperparameters so that you can reload the model more easily\n",
    "        time_stamp = time.strftime(\"%m%d%H%M%S\")#month,day,hour,minute,second\n",
    "        self.model_path = f'models/{model_name}_{time_stamp}.pt'\n",
    "\n",
    "        self.losses = {split: [] for split in ['train', 'eval', 'test']}\n",
    "        \n",
    "    def compute_loss(self, x, y, model=None):\n",
    "        y = y.view(-1)\n",
    "        y_pred = self.model(x)\n",
    "        y_pred = y_pred.view(-1)\n",
    "        loss = self.criterion(y, y_pred)\n",
    "        return loss, y_pred, y\n",
    "    \n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        # batch losses\n",
    "        b_losses = []\n",
    "        for x, y in loader:\n",
    "            # Setting the optimizer gradient to Zero\n",
    "            self.optimizer.zero_grad()\n",
    "            x.to(torch.device(self.device))\n",
    "            y.to(torch.device(self.device))\n",
    "            \n",
    "            loss, pred, target = self.compute_loss(x, y)\n",
    "            \n",
    "            # Backpropagate the training loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            b_losses.append(loss.detach().numpy())\n",
    "        \n",
    "        # aggregated losses across batches\n",
    "        agg_loss = np.sqrt((np.asarray(b_losses) ** 2).mean())\n",
    "        self.losses['train'].append(agg_loss)\n",
    "        return agg_loss\n",
    "\n",
    "    # decorator, equivalent to with torch.no_grad():\n",
    "    @torch.no_grad()\n",
    "    def eval_epoch(self, loader, split='eval'):\n",
    "        self.model.eval()\n",
    "        # batch losses\n",
    "        b_losses = []\n",
    "        for x, y in loader:\n",
    "            x.to(torch.device(self.device))\n",
    "            y.to(torch.device(self.device))\n",
    "            \n",
    "            loss, pred, target = self.compute_loss(x, y)\n",
    "            \n",
    "            b_losses.append(loss.detach().numpy())\n",
    "        \n",
    "        # aggregated losses across batches\n",
    "        agg_loss = np.sqrt((np.asarray(b_losses) ** 2).mean())\n",
    "        self.losses[split].append(agg_loss)\n",
    "        return agg_loss\n",
    "        \n",
    "    def fit(self, loaders):\n",
    "        print(f\"Training model for {self.n_epochs} epochs...\")\n",
    "        train_loader, eval_loader, test_loader = loaders\n",
    "        train_start = time.time()\n",
    "        \n",
    "        start_epoch = 0\n",
    "        best_eval_loss = np.inf\n",
    "\n",
    "        ##############\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        ##############\n",
    "            \n",
    "        for epoch in range(start_epoch, self.n_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            eval_loss = self.eval_epoch(eval_loader, split='eval')\n",
    "            test_loss = self.eval_epoch(test_loader, split='test')\n",
    "\n",
    "            if eval_loss < best_eval_loss:\n",
    "                best_eval_loss = eval_loss\n",
    "                self.save(self.model, self.model_path)\n",
    "                ##############\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                ##############\n",
    "\n",
    "            s = (\n",
    "                f\"[Epoch {epoch + 1}] \"\n",
    "                f\"train_loss = {train_loss:.5f}, \"\n",
    "                f\"eval_loss = {eval_loss:.5f}, \"\n",
    "                f\"test_loss = {test_loss:.5f}\"\n",
    "            )\n",
    "\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            s += f\" [{epoch_time:.1f}s]\"\n",
    "            print(s)\n",
    "\n",
    "            ##############\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered.\")\n",
    "                train_time = int(time.time() - train_start)\n",
    "                break\n",
    "            ##############\n",
    "    \n",
    "        train_time = int(time.time() - train_start)\n",
    "                \n",
    "        print(f'Task done in {train_time}s')\n",
    "    \n",
    "    @ staticmethod\n",
    "    def evaluate_model_performance(y_pred, y_true):\n",
    "        \"\"\"\n",
    "        ## Task: Define Model Performance on RMSE and NASA score\n",
    "        The performance of your implemented model should be evaluated using two common metrics applied in N-CMAPSS prognostics analysis:\n",
    "        RMSE and NASA-score (in 1E5) as introduced in [\"Fusing Physics-based and Deep Learning Models for Prognostics\"](https://arxiv.org/abs/2003.00732)\n",
    "        \"\"\"\n",
    "        # TODO: add implementation\n",
    "        # Compute NASA score\n",
    "        diff = y_pred - y_true\n",
    "        alpha = np.where(diff > 0, 1/10, 1/13)\n",
    "\n",
    "        # Calculate the NASA score based on the penalty factor and the magnitude of the error\n",
    "        score = np.sum(np.exp(alpha * np.abs(diff)))\n",
    "\n",
    "        # Normalize the NASA score as required\n",
    "        score = score / 1e5\n",
    "\n",
    "        # Compute RMSE (Root Mean Square Error)\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "        return score, rmse\n",
    "    \n",
    "    # decorator, equivalent to with torch.no_grad():\n",
    "    @torch.no_grad()\n",
    "    def eval_rul_prediction(self, test_loader):\n",
    "        print(f\"Evaluating test RUL...\")\n",
    "        \n",
    "        ## MASK OUT EVAL and add explanation\n",
    "        best_model = self.load(self.model) \n",
    "        best_model.eval()\n",
    "        \n",
    "        preds = []\n",
    "        trues = []\n",
    "        \n",
    "        for x, y in tqdm(test_loader):\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            _, y_pred, y_target = self.compute_loss(x, y)\n",
    "            preds.append(y_pred.detach().cpu().numpy())\n",
    "            trues.append(y_target.detach().cpu().numpy())\n",
    "        \n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        \n",
    "        df = pd.DataFrame({         \n",
    "            'pred': preds,\n",
    "            'true': trues,\n",
    "            'err': np.sqrt((preds - trues)**2)\n",
    "        })\n",
    "        \n",
    "        score, rmse = self.evaluate_model_performance(preds, trues)\n",
    "        df_out = pd.DataFrame({\n",
    "            'score': [score],\n",
    "            'rmse': [rmse],\n",
    "            'seed': [self.seed],\n",
    "        })\n",
    "        return df, df_out\n",
    "\n",
    "    def save(self, model, model_path=None):\n",
    "        os.makedirs(f'{folder}/models', exist_ok=True)\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "    def load(self, model, model_path=None):\n",
    "        \"\"\"\n",
    "        loads the prediction model's parameters\n",
    "        \"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        print(f\"Model {model.__class__.__name__} saved in {model_path} loaded to {self.device}\")\n",
    "        return model\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        :param losses: dict with losses\n",
    "        \"\"\"\n",
    "        linestyles = {\n",
    "            'train': 'solid', \n",
    "            'eval': 'dashed', \n",
    "            'test': 'dotted', \n",
    "        }\n",
    "        for split, loss in self.losses.items():\n",
    "            ls = linestyles[split]\n",
    "            plt.plot(range(1, 1+len(loss)), loss, label=f'{split} loss', linestyle=ls)\n",
    "            plt.yscale('log')\n",
    "                \n",
    "        plt.title(\"Training/Validation Losses\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igknd0rYN56n"
   },
   "source": [
    "# Task: Implement a 1D Convolutional Neural Network (1DCNN) Model\n",
    "Conventional CNN's developed for image tasks learn to extract features from the 2D input data. They are autonomous (require no domain expertise or prior info about the image) and thus can be applied to any image regardless of its dimensions. This is due to the fact that these CNN's go through an image by downsampling the image which we call straddling or windowing.  \n",
    "\n",
    "Similarly 1D CNN learns to extract features from a time series data, by windowing over the data, considering a set of data observations each time. The benefit of using the CNN for sequence classification is that it can learn from the raw time series data, and in turn do not require domain expertise to engineer relevant features.\n",
    "\n",
    "The CNN architecture outlined in the paper consists of five layers. The initial three layers are convolutional layers, each employing filters of size 10. The first two convolutional layers consist of ten channels each, while the third convolutional layer comprises a single channel. Zero padding is utilized to maintain the dimensions of the feature map throughout the network. Following the convolutional layers, the 2D feature map is flattened, leading into a 50-unit fully connected layer, and subsequently, a linear output neuron. The activation function used across the network is ReLU. The network encompasses 24,000 trainable parameters.\n",
    "\n",
    "As an advanced extension of this task, you are encouraged to explore the inclusion of Dropout and Batch Normalization layers as regularization techniques to improve the model's generalization performance. Implementing these additional layers can help in reducing overfitting and ensuring that the model generalizes well to unseen data. Your exploration should evaluate the impact of these regularization techniques on the model's performance and compare the results with the baseline model (without Dropout and Batch Normalization).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/assignment_1dcnn.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K831VUjPLSlC"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.BatchNorm1d):\n",
    "        m.weight.data.fill_(1.0)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        m.weight.data = nn.init.xavier_uniform_(\n",
    "            m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    A 1D-CNN model that is based on the paper \"Fusing physics-based and deep learning models for prognostics\"\n",
    "    from Manuel Arias Chao et al. (with batchnorm layers)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=14, #corresponds to the number of sensor readings\n",
    "                 out_channels=1, #RUL\n",
    "                 window= 50 ,\n",
    "                 n_ch=10, \n",
    "                 n_k=10, \n",
    "                 n_hidden=50, \n",
    "                 n_layers=3,\n",
    "                 dropout=0.,\n",
    "                 batch_norm = False,\n",
    "                 padding='same'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_features (int, optional): number of input features. Defaults to 18.\n",
    "            window (int, optional): sequence length. Defaults to 50.\n",
    "            n_ch (int, optional): number of channels (filter size). Defaults to 10.\n",
    "            n_k (int, optional): kernel size. Defaults to 10.\n",
    "            n_hidden (int, optional): number of hidden neurons for regressor. Defaults to 50.\n",
    "            n_layers (int, optional): number of convolution layers. Defaults to 5.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: implement model architecture\n",
    "        # Add convolution layers with BatchNorm and ReLU activations\n",
    "        ######################\n",
    "        # layers = []\n",
    "        # for i in range(n_layers - 1):\n",
    "        #     layers.append(nn.Conv1d(in_channels=in_channels if i == 0 else n_ch, \n",
    "        #                             out_channels=n_ch, \n",
    "        #                             kernel_size=n_k, \n",
    "        #                             padding=padding))\n",
    "        #     layers.append(nn.ReLU())\n",
    "        #     layers.append(nn.BatchNorm1d(n_ch))\n",
    "        \n",
    "        # # The third layer should have only one output channel (for the single-channel feature map)\n",
    "        # layers.append(nn.Conv1d(in_channels=n_ch, \n",
    "        #                         out_channels=1, \n",
    "        #                         kernel_size=n_k, \n",
    "        #                         padding=padding))\n",
    "        # layers.append(nn.ReLU())\n",
    "        # layers.append(nn.BatchNorm1d(1))\n",
    "        \n",
    "        # # Convert the layers into a sequential block\n",
    "        # self.conv_layers = nn.Sequential(*layers) # * to pass each layer separately \n",
    "\n",
    "        # # Define the fully connected layers\n",
    "        # self.fc1 = nn.Linear(window, n_hidden)  # Input size is the flattened window size\n",
    "        # self.fc2 = nn.Linear(n_hidden, out_channels)  # Output layer for regression (single output)\n",
    "\n",
    "        # self.apply(init_weights)\n",
    "        ######################\n",
    "        ######################\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, \n",
    "                      out_channels=n_ch,\n",
    "                      kernel_size=n_k,\n",
    "                      padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(n_ch) if batch_norm else nn.Identity())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=n_ch, \n",
    "                      out_channels=n_ch,\n",
    "                      kernel_size=n_k,\n",
    "                      padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(n_ch) if batch_norm else nn.Identity()) \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=n_ch, \n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=n_k,\n",
    "                      padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1) if batch_norm else nn.Identity())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(window, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden,out_channels))\n",
    "        self.apply(init_weights)\n",
    "        ##########################\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data of shape (batch_size, in_channels, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output prediction for RUL (Remaining Useful Life)\n",
    "        \"\"\"\n",
    "        # TODO: implement forward pass\n",
    "        ######################\n",
    "        # # 3 convolutional layers \n",
    "        # x = self.conv_layers(x)\n",
    "\n",
    "        # # Flatten the output of the last convolutional layer\n",
    "        # x = x.view(x.size(0), -1)\n",
    "\n",
    "        # #################\n",
    "        # # for me the flatten part should be between the last two layers and not before\n",
    "        # # and I am not convince by the two last line \n",
    "        # #################\n",
    "\n",
    "        # # Fully connected layers\n",
    "        # x = F.relu(self.fc1(x)) # straightforward activation function to tensors\n",
    "        # x = self.fc2(x)\n",
    "        ######################\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-JGs7gqhn8w"
   },
   "source": [
    "# Training a model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84R7i9gF4KFD"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    r\"\"\"Sets the seed for generating random numbers in PyTorch, numpy and\n",
    "    Python.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The desired seed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ2ePKogyvCq"
   },
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "TRAIN_UNITS = [2, 5, 10, 16, 18, 20]\n",
    "TEST_UNITS = [11, 14, 15]\n",
    "\n",
    "DEFAULT_PARAMS = {\n",
    "    # CNN model parameters\n",
    "    'in_channels': 18, \n",
    "    'out_channels': 1,\n",
    "    'window': 50,\n",
    "    'n_ch': 10, \n",
    "    'n_k': 10, \n",
    "    'n_hidden': 50, \n",
    "    'n_layers': 3,\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0., \n",
    "    'padding': 'same', \n",
    "    # training parameters\n",
    "    'batch_size': 256,\n",
    "    'base_lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'max_epochs': 1 #50\n",
    "}\n",
    "\n",
    "DATASETS = create_datasets(df, window_size=DEFAULT_PARAMS['window'], train_units=TRAIN_UNITS, test_units=TEST_UNITS)\n",
    "LOADERS = create_data_loaders(DATASETS, batch_size=DEFAULT_PARAMS['batch_size'], val_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WqMyBWqyk9U",
    "outputId": "e3f3b51b-8a0b-4cbf-844d-1f4957d62e33"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "model = CNN(\n",
    "  in_channels=DEFAULT_PARAMS['in_channels'],\n",
    "  out_channels=DEFAULT_PARAMS['out_channels'],\n",
    "  n_ch=DEFAULT_PARAMS['n_ch'],\n",
    "  n_k=DEFAULT_PARAMS['n_k'],\n",
    "  n_hidden=DEFAULT_PARAMS['n_hidden'],\n",
    "  n_layers=DEFAULT_PARAMS['n_layers'],\n",
    "  batch_norm=DEFAULT_PARAMS['batch_norm']\n",
    "  dropout=DEFAULT_PARAMS['dropout'],\n",
    ")\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "  model.parameters(),\n",
    "  lr=DEFAULT_PARAMS['base_lr'],\n",
    "  weight_decay=DEFAULT_PARAMS['weight_decay'],\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  optimizer,\n",
    "  criterion=criterion,\n",
    "  n_epochs=DEFAULT_PARAMS['max_epochs'],\n",
    "  seed=SEED,\n",
    ")\n",
    "\n",
    "trainer.fit(LOADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "IkwLJMN-sADH",
    "outputId": "1a94ff11-4367-42d6-8263-b4c04200d2d5"
   },
   "outputs": [],
   "source": [
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "fPONTMrK7t2P",
    "outputId": "a0a9ebbb-f5b8-4b27-a91d-150f837d6595"
   },
   "outputs": [],
   "source": [
    "df_test, df_out = trainer.eval_rul_prediction(LOADERS[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "SXyGipjP77TV",
    "outputId": "5298e72a-ab4c-4859-c291-b1f084f4e7ec"
   },
   "outputs": [],
   "source": [
    "df_test.plot(y=['true', 'pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Dropout and Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA GENERAL\n",
    "\n",
    "params = {\n",
    "    # CNN model parameters\n",
    "    'in_channels': 18, \n",
    "    'out_channels': 1,\n",
    "    'window': 50,\n",
    "    'n_ch': 10, \n",
    "    'n_k': 10, \n",
    "    'n_hidden': 50, \n",
    "    'n_layers': 3,\n",
    "    'batch_norm':False, # to be changed\n",
    "    'dropout': 0., # to be changed\n",
    "    'padding': 'same', \n",
    "    # training parameters\n",
    "    'batch_size': 256,\n",
    "    'base_lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'max_epochs': 3 \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Experiment loop, we remove the default setting \n",
    "configurations = [\n",
    "    {\"batch_norm\": True, \"dropout\": 0.0},      # Batch norm only\n",
    "    {\"batch_norm\": False, \"dropout\": 0.1},     # Dropout only (e.g., 0.3)\n",
    "    {\"batch_norm\": True, \"dropout\": 0.1},       # Both dropout and batch norm\n",
    "    {\"batch_norm\": False, \"dropout\": 0.2},     \n",
    "    {\"batch_norm\": True, \"dropout\": 0.2},   \n",
    "    {\"batch_norm\": False, \"dropout\": 0.3},     \n",
    "    {\"batch_norm\": True, \"dropout\": 0.3},\n",
    "    {\"batch_norm\": False, \"dropout\": 0.4},     \n",
    "    {\"batch_norm\": True, \"dropout\": 0.4},  \n",
    "    {\"batch_norm\": False, \"dropout\": 0.5},     \n",
    "    {\"batch_norm\": True, \"dropout\": 0.5}  \n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configurations:\n",
    "    print(f\"Running configuration: BatchNorm={config['batch_norm']}, Dropout={config['dropout']}\")\n",
    "    seed_everything(SEED)\n",
    "    model = CNN(\n",
    "        in_channels=params['in_channels'],\n",
    "        out_channels=params['out_channels'],\n",
    "        n_ch=params['n_ch'],\n",
    "        n_k=params['n_k'],\n",
    "        n_hidden=params['n_hidden'],\n",
    "        n_layers=params['n_layers'],\n",
    "        batch_norm=config['batch_norm'],\n",
    "        dropout=config[\"dropout\"], # here I change the drop out to the one in config\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=params['base_lr'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion=criterion,\n",
    "        n_epochs=params['max_epochs'],\n",
    "        seed=SEED,\n",
    "    )\n",
    "    trainer.fit(LOADERS)\n",
    "\n",
    "    df_eval, df_eval_out = trainer.eval_rul_prediction(LOADERS[1])\n",
    "    df_test, df_test_out = trainer.eval_rul_prediction(LOADERS[2])\n",
    "\n",
    "    # extract the metrics of the eval \n",
    "    results.append({\n",
    "        \"config\": config,\n",
    "        \"score\": df_eval_out['score'].values[0],\n",
    "        \"rmse\": df_eval_out['rmse'].values[0]\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Sort the configurations by RMSE (ascending order for better performance)\n",
    "sorted_results = sorted(results, key=lambda x: x['rmse'])\n",
    "\n",
    "# Get the top 3 configurations based on RMSE\n",
    "top_three_configs = sorted_results[:3]\n",
    "\n",
    "\n",
    "params_combination_1 = {\n",
    "    # CNN model parameters\n",
    "    'in_channels': 18, \n",
    "    'out_channels': 1,\n",
    "    'window': 50,\n",
    "    'n_ch': 10, \n",
    "    'n_k': 10, \n",
    "    'n_hidden': 50, \n",
    "    'n_layers': 3,\n",
    "    'batch_norm': top_three_configs[0]['config']['batch_norm'], \n",
    "    'dropout': top_three_configs[0]['config']['dropout'], \n",
    "    'padding': 'same', \n",
    "    # training parameters\n",
    "    'batch_size': 256,\n",
    "    'base_lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'max_epochs': 50  # You can adjust this as needed\n",
    "}\n",
    "\n",
    "params_combination_2 = {\n",
    "    # CNN model parameters\n",
    "    'in_channels': 18, \n",
    "    'out_channels': 1,\n",
    "    'window': 50,\n",
    "    'n_ch': 10, \n",
    "    'n_k': 10, \n",
    "    'n_hidden': 50, \n",
    "    'n_layers': 3,\n",
    "    'batch_norm': top_three_configs[1]['config']['batch_norm'], \n",
    "    'dropout': top_three_configs[1]['config']['dropout'], \n",
    "    'padding': 'same', \n",
    "    # training parameters\n",
    "    'batch_size': 256,\n",
    "    'base_lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'max_epochs': 50  # You can adjust this as needed\n",
    "}\n",
    "\n",
    "params_combination_3 = {\n",
    "    # CNN model parameters\n",
    "    'in_channels': 18, \n",
    "    'out_channels': 1,\n",
    "    'window': 50,\n",
    "    'n_ch': 10, \n",
    "    'n_k': 10, \n",
    "    'n_hidden': 50, \n",
    "    'n_layers': 3,\n",
    "    'batch_norm': top_three_configs[2]['config']['batch_norm'], \n",
    "    'dropout': top_three_configs[2]['config']['dropout'], \n",
    "    'padding': 'same', \n",
    "    # training parameters\n",
    "    'batch_size': 256,\n",
    "    'base_lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'max_epochs': 50  # You can adjust this as needed\n",
    "}\n",
    "\n",
    "params_combinations = [params_combination_1, params_combination_2, params_combination_3]\n",
    "\n",
    "results_long_run = []\n",
    "for parameters_ in params_combinations:\n",
    "    seed_everything(SEED)\n",
    "    model = CNN(\n",
    "        in_channels=parameters_['in_channels'],\n",
    "        out_channels=parameters_['out_channels'],\n",
    "        n_ch=parameters_['n_ch'],\n",
    "        n_k=parameters_['n_k'],\n",
    "        n_hidden=parameters_['n_hidden'],\n",
    "        n_layers=parameters_['n_layers'],\n",
    "        batch_norm=parameters_['batch_norm'],\n",
    "        dropout=parameters_[\"dropout\"], \n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=parameters_['base_lr'],\n",
    "        weight_decay=parameters_['weight_decay'],\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion=criterion,\n",
    "        n_epochs=parameters_['max_epochs'],\n",
    "        seed=SEED,\n",
    "    )\n",
    "    trainer.fit(LOADERS)\n",
    "\n",
    "    df_eval, df_eval_out = trainer.eval_rul_prediction(LOADERS[1])\n",
    "    df_test, df_test_out = trainer.eval_rul_prediction(LOADERS[2])\n",
    "\n",
    "    # extract the metrics of the eval \n",
    "    results_long_run.append({\n",
    "        \"config\": config,\n",
    "        \"score\": df_eval_out['score'].values[0],\n",
    "        \"rmse\": df_eval_out['rmse'].values[0]\n",
    "    })\n",
    "print(results_long_run)\n",
    "indexed_scores = list(enumerate(results_long_run))\n",
    "sorted_scores = sorted(indexed_scores, key=lambda x: x[1]['rmse'])\n",
    "sorted_results_and_index = sorted(results_long_run, key=lambda x: x['rmse'])\n",
    "top_indices = [index for index, score in sorted_scores[:3]]\n",
    "best_index = top_indices[0]\n",
    "BEST_PARAMS = params_combinations[best_index]\n",
    "print(BEST_PARAMS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate results of multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this model with the most relevent previous parameter\n",
    "def run_single(seed, params=BEST_PARAMS):\n",
    "  seed_everything(seed)\n",
    "\n",
    "  model = CNN(\n",
    "    in_channels=params['in_channels'],\n",
    "    out_channels=params['out_channels'],\n",
    "    n_ch=params['n_ch'],\n",
    "    n_k=params['n_k'],\n",
    "    n_hidden=params['n_hidden'],\n",
    "    n_layers=params['n_layers'],\n",
    "    batch_norm=params['batch_norm'],\n",
    "    dropout=params['dropout'],\n",
    "  )\n",
    "\n",
    "  optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=params['base_lr'],\n",
    "    weight_decay=params['weight_decay'],\n",
    "  )\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  trainer = Trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion=criterion,\n",
    "    n_epochs=params['max_epochs'],\n",
    "    seed=seed,\n",
    "  )\n",
    "\n",
    "  trainer.fit(LOADERS)\n",
    "  df_eval, df_eval_out = trainer.eval_rul_prediction(LOADERS[1])\n",
    "  df_test, df_test_out = trainer.eval_rul_prediction(LOADERS[2])\n",
    "  return df_eval, df_eval_out, df_test, df_test_out\n",
    "\n",
    "N_RUNS = 5\n",
    "eval_score = []\n",
    "eval_rmse = []\n",
    "test_score = []\n",
    "test_rmse = []\n",
    "predictions_df = pd.DataFrame()  # DataFrame to store errors with columns for each run\n",
    "\n",
    "for i in range(SEED, SEED+N_RUNS):\n",
    "  df_eval, df_eval_out, df_test, df_test_out = run_single(i)\n",
    "\n",
    "  eval_score.append(df_eval_out['score'])\n",
    "  eval_rmse.append(df_eval_out['rmse'])\n",
    "  test_score.append(df_test_out['score'])\n",
    "  test_rmse.append(df_test_out['rmse'])\n",
    "\n",
    "  # Calculate the prediction for df_test and accumulate them\n",
    "  predictions = df_test['pred']-df_test['true']  # Calculate prediction\n",
    "  predictions_df[f'#run{i+1}'] = predictions.values   # Append predictions to cumulative array\n",
    "\n",
    "\n",
    "\n",
    "# Compute mean and standard deviation for RMSE and score\n",
    "\n",
    "# Function to calculate mean and standard deviation\n",
    "def calc_stats(data):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    return mean, std_dev\n",
    "\n",
    "# Calculate for each list\n",
    "eval_score_mean, eval_score_std = calc_stats(eval_score)\n",
    "eval_rmse_mean, eval_rmse_std = calc_stats(eval_rmse)\n",
    "test_score_mean, test_score_std = calc_stats(test_score)\n",
    "test_rmse_mean, test_rmse_std = calc_stats(test_rmse)\n",
    "\n",
    "# Print results\n",
    "print(f\"Evaluation Score - Mean: {eval_score_mean}, Std Dev: {eval_score_std}\")\n",
    "print(f\"Evaluation RMSE - Mean: {eval_rmse_mean}, Std Dev: {eval_rmse_std}\")\n",
    "print(f\"Test Score - Mean: {test_score_mean}, Std Dev: {test_score_std}\")\n",
    "print(f\"Test RMSE - Mean: {test_rmse_mean}, Std Dev: {test_rmse_std}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the performence per unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Identify change indices in `df_test['true']`\n",
    "change_indices = df_test.index[(df_test['true'].shift(1) == 0) & (df_test['true'] > 0)].tolist()\n",
    "\n",
    "# Add the first and last index to cover all intervals\n",
    "split_indices = np.concatenate(([0], change_indices, [len(df_test)]))\n",
    "\n",
    "# Cycle indices\n",
    "cycle_indices = np.where(df_test['true'].diff() != 0)[0]\n",
    "cycle_indices = np.concatenate(( cycle_indices, [len(df_test)]))\n",
    "\n",
    "# Step 2: Separate `predictions_df` into a dictionary based on unique intervals\n",
    "predictions_dict_unit = {i: predictions_df.iloc[split_indices[i]:split_indices[i+1]] for i in range(len(split_indices) - 1)}\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Step 3: Calculate and plot mean, standard deviation, and confidence intervals for each unit segment\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Define colors for each unit\n",
    "\n",
    "for index, predictions_df_unit in predictions_dict_unit.items():\n",
    "    # Calculate mean and confidence interval for each point in the segment\n",
    "    mean_prediction = predictions_df_unit.mean(axis=1)  # Mean prediction at each point\n",
    "    std_prediction = predictions_df_unit.std(axis=1)    # Standard deviation at each point\n",
    "    \n",
    "    confidence_interval = 1.96 * std_prediction / np.sqrt(N_RUNS)  # 95% confidence interval\n",
    "    \n",
    "    # filter the cycle_indices\n",
    "    predictions_range_start = predictions_df_unit.index[0]\n",
    "    predictions_range_end = predictions_df_unit.index[-1]\n",
    "    cycle_indices_in_range = [index for index in cycle_indices if predictions_range_start <= index <= predictions_range_end+1]\n",
    "    cycle_indices_in_range[-1] -= 1 # manual correction\n",
    "    # Assuming cycle_indices_in_range and predictions_df_unit are already defined\n",
    "    mean_predictions = []\n",
    "    min_interval_predictions = []\n",
    "    max_interval_predictions = []\n",
    "    # Loop through cycle indices to compute means for each segment\n",
    "    for i in range(len(cycle_indices_in_range) - 1):\n",
    "        start_index = cycle_indices_in_range[i]\n",
    "        end_index = cycle_indices_in_range[i + 1]\n",
    "        \n",
    "        # Slice predictions_df_unit based on the indices\n",
    "        # Ensure that you slice only within the bounds of the DataFrame\n",
    "        segment = predictions_df_unit.loc[start_index:end_index - 1]  # End index is exclusive\n",
    "        segment_mean = mean_prediction.loc[start_index:end_index - 1]\n",
    "        segment_confidence_interval = confidence_interval.loc[start_index:end_index - 1]\n",
    "        \n",
    "        \n",
    "        # Compute the mean for the current segment\n",
    "        mean_segment = segment_mean.mean() # Get the mean value (assuming 1 column)\n",
    "        mean_predictions.append(mean_segment)\n",
    "        std_prediction = segment.std().values[0] # Standard deviation at each point\n",
    "        min_interval_segment =  min(segment_mean - segment_confidence_interval)\n",
    "        \n",
    "        max_interval_segment =  max(segment_mean + segment_confidence_interval)\n",
    "        min_interval_predictions.append(min_interval_segment)\n",
    "        max_interval_predictions.append(max_interval_segment)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    # Define x values starting from 0 for each segment\n",
    "    x_values = np.arange(len(mean_predictions))\n",
    "\n",
    "    # Plot mean prediction with confidence interval for the current segment\n",
    "    unit_label = f\"Unit {TEST_UNITS[index]}\"  # Label for the current unit\n",
    "    plt.plot(x_values, mean_predictions, label=f'{unit_label}', color=colors[index], alpha=0.9, linewidth=1.5, marker='o', markersize=4)  \n",
    "    plt.fill_between(\n",
    "        x_values,\n",
    "        min_interval_predictions,\n",
    "        max_interval_predictions,\n",
    "        color=colors[index],\n",
    "        alpha=0.2  # Keep the confidence interval fill transparency\n",
    "    )    \n",
    "\n",
    "# Add ¬±5 horizontal lines with a dashed red line\n",
    "plt.axhline(y=5, color='red', linestyle='-.', linewidth=1, label=r'$\\epsilon = \\pm 5$')\n",
    "plt.axhline(y=-5, color='red', linestyle='-.', linewidth=1)\n",
    "\n",
    "# Add a solid line at 0 for reference\n",
    "plt.axhline(y=0, color='red', linestyle='-', linewidth=2)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel('Time [cycles]')\n",
    "plt.ylabel('Error RUL [cycles]')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M3V0Q1rUTU3"
   },
   "source": [
    "## Bonus: Hyperparameter tuning\n",
    "\n",
    "**Model parameters:** These are the parameters that are estimated by the model from the given data. For example the weights of a deep neural network. \n",
    "\n",
    "**Model hyperparameters:** These are the parameters that cannot be estimated by the model from the given data. These parameters are used to estimate the model parameters. For example, the learning rate in deep neural networks.\n",
    "\n",
    "*Hyperparameter tuning* (or hyperparameter optimization) is the process of determining the right combination of hyperparameters that maximizes the model performance. It works by running multiple trials in a single training process. Each trial is a complete execution of your training application with values for your chosen hyperparameters, set within the limits you specify. This process once finished will give you the set of hyperparameter values that are best suited for the model to give optimal results.\n",
    "\n",
    "There are many python libraries for hyperparameter tuning:\n",
    "1. RayTune\n",
    "2. Optuna\n",
    "3. Hyperopt\n",
    "4. sklearn\n",
    "etc.\n",
    "\n",
    "Here we will build a simple tuner using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDsveVb13ka7",
    "outputId": "b2d762f7-a69e-4fd1-8f79-819d1e5a0751"
   },
   "outputs": [],
   "source": [
    "!pip install -U optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLe-fLs58hGh"
   },
   "source": [
    "There are several ways of hyperparameter tuning (all the above mentioned libraries apply several or all of those):\n",
    "\n",
    "GridSearch : We create a discrete search space of hyperparameters to be tuned. This descrete search space grid is exhaustively searched for the best combination.\n",
    "\n",
    "RandomSearch : We define distributions for each hyperparameter. Here the key difference is not all values are tested and values are selected at random. Since randomsearch does not test all hyperparameter values it does not necessarily return the best performing parameters, but it returns a good performing model in significantly shorter time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIOV4QmF7jJg"
   },
   "source": [
    "This is an example of random search, Optuna Tune uses Bayesian optimization by default, however for this time we use the random search algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ38y2Ev3Aag"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(params):\n",
    "    n_runs = 3 # number of runs to average over, you can decrease this number to speed up the optimization\n",
    "    df_all_val = pd.DataFrame()\n",
    "    for i in n_runs:\n",
    "        seed = SEED + i\n",
    "        df_eval, df_eval_out, df_test, df_test_out = run_single(seed, params)\n",
    "        df_all_val = df_all_val.append(df_eval_out) # append the validation results\n",
    "    # average over n_runs of the validation results, we use the mean of the rmse as the objective to minimize\n",
    "    # we use the validation results for hyperparameter tuning\n",
    "    rmse = df_all_val['rmse'].mean()\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJBN4I6s3AVZ"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # We can add all the parameters that use for defining the model and trainer or even the dataset builder.\n",
    "\n",
    "    # we define the parameter space, you can add more parameters to tune or reduce, you can also start with the best hyperparameters defined in the paper and finetune\n",
    "    params = DEFAULT_PARAMS.copy()\n",
    "    params['base_lr'] = trial.suggest_categorical('base_lr', [0.001, 0.01, 0.1])\n",
    "    params['n_layer'] = trial.suggest_categorical('kernel_size ', [3, 4, 5])\n",
    "    params['kernel_size'] = trial.suggest_categorical('kernel_size ', [3, 5, 7])\n",
    "    ...\n",
    "    \n",
    "    out = evaluate_model(params)\n",
    "    # We need to minimize the predicted rmse.\n",
    "    rmse = out['rmse']\n",
    "    return rmse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a study name, otherwise a random name will be generated\n",
    "study_name = 'hypertune_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBO1_BFl3Adi",
    "outputId": "956c1b84-2956-449e-bf22-9fe2d9086cd0"
   },
   "outputs": [],
   "source": [
    "n_trials = 2 # number of set of hyperparameters to train\n",
    "# based on the output of the objective, we either maximize or minimize. If we returned accuracy, we would be maximize.    \n",
    "study = optuna.create_study(f'sqlite:///{folder}/study.db',\n",
    "                            study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            load_if_exists=True,)\n",
    "study.optimize(objective, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c92SU6slDQli"
   },
   "outputs": [],
   "source": [
    "# Load the study for resuming, comment out when reloading\n",
    "# study = optuna.load_study(study_name=study_name, storage=f'sqlite:///{folder}/study.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing impact of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can visualize the importance of hyperparameters\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot between two hyperparameters\n",
    "fig = optuna.visualization.plot_contour(study, params=['lr', 'nl'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get a dataframe of the hyperparamter results with:\n",
    "hyper_df = study.trials_dataframe()\n",
    "hyper_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vs-w0nYUAcMB",
    "outputId": "b4677ceb-db36-4f73-8378-09c150d47cff"
   },
   "outputs": [],
   "source": [
    "# Get the est model parameter\n",
    "best_trial = study.best_trial\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Prognostics for Turbofine Engines with 1D Convolutional Neural Networks ü©∫\n",
    "\n",
    "## I. Task Description:\n",
    "\n",
    "### Background:\n",
    "Conventional CNNs, primarily developed for image analysis, have shown immense capability in autonomously extracting features from 2D data. This autonomy, achieved through striding or windowing, enables the application of CNNs across various image dimensions without requiring domain expertise. \n",
    "\n",
    "Similarly, 1D CNNs have demonstrated effectiveness in time series data analysis by employing a windowing technique to scan through sequential data, extracting valuable features autonomously. This characteristic of 1D CNNs makes them a powerful tool for prognostics, a field concerned with predicting the Remaining Useful Life (RUL) of systems based on historical and real-time operational data.\n",
    "\n",
    "### Problem Formulation üöÄ:\n",
    "In this task, the objective is to delve into the domain of prognostics to address a real-world problem. Utilizing the architecture described in the paper: \"Fusing Physics-based and Deep Learning Models for Prognostics\" ([link](https://arxiv.org/abs/2003.00732)), you have to develop a 1D CNN model to predict the Remaining Useful Life (RUL) of certain units. The provided dataset comprises time series data from different units, and your goal is to accurately estimate the RUL for specified test units.\n",
    "\n",
    "Training units (engines) **2, 5, 10, 16, 18, 20** \n",
    "\n",
    "Test units (engines) **11, 14, 15** \n",
    "\n",
    "### Exploratory Data Analysis:\n",
    "1. Explore and understand the provided dataset, identifying the key features that could be indicative of a unit's health and consequently its RUL **(10 points max)**.\n",
    "\n",
    "\n",
    "### Implementation:\n",
    "1. Implement the 1D CNN model as per the referenced paper **(20 points max)**. Be careful with the tensor dimensions that you pass to 1DCNN. It should have (n samples, n channels, n timesteps) dimensions.\n",
    "2. Investigate the effect of Dropout and Batch Normalization layers on the model's performance as regularization techniques to mitigate overfitting **(10 points max)**.\n",
    "\n",
    "### Interpretation:\n",
    "1. How do the predictions of RUL evolve over time, within a cycle (i.e a complete flight cycle consisting of taking off, crusing and landing) and between cycles, for a given unit? Visualize and discuss **(30 points max)**.\n",
    "2. How well does your model generalizes to the test units? Identify possible reasons why the model may perform better on some units and worse on others **(20 points max)**. \n",
    "\n",
    "### Evaluation:\n",
    "The performance of your implemented model should be rigorously evaluated across 5 independent runs with different random seeds to ensure the robustness of the results. \n",
    "For each run, utilize the following two common metrics applied in C-MAPSS prognostics analysis:\n",
    "\n",
    "The performance of your implemented model should be evaluated using two common metrics applied in C-MAPSS prognostics analysis:\n",
    "1. **Root Mean Square Error (RMSE)**:\n",
    "\n",
    "$\\text{RMSE} = \\sqrt{\\frac{1}{m^*} \\sum_{j=1}^{m^*} (\\Delta(j))^2}$\n",
    "\n",
    "where $m^*$ denotes the total number of test data samples, and $\\Delta(j)$ is the difference between the estimated and the real Remaining Useful Life (RUL) of the $j$-th sample, i.e., $y(j) - \\hat{y}(j)$.\n",
    "\n",
    "2. **NASA's Scoring Function (s)**:\n",
    "$s = \\sum_{j=1}^{m^*} \\exp\\left(\\alpha \\cdot |\\Delta(j)|\\right) $\n",
    "where $\\alpha = \\frac{1}{13}$ if RUL is underestimated, and $\\alpha = \\frac{1}{10}$ otherwise. The scoring function $s$ is not symmetric and penalizes over-estimation more than under-estimation.\n",
    "\n",
    "\n",
    "Aggregate the results from all **5 runs** to provide a mean and standard deviation for both the RMSE and NASA's Scoring Function. This aggregated evaluation will provide a more robust understanding of the model's performance on the selected prognostics task, comparing it to purely data-driven deep learning models as outlined in the referenced paper.\n",
    "\n",
    "We provide in total **(10 points max)** for proper evaluation of the model. \n",
    "\n",
    "### Bonus:\n",
    "1. Apply hyperparameter tuning with grid search to improve the model performance. You can start with the best hyperparameters from the paper **(10 points max)**. \n",
    "2. Explore alternative architectures or approaches to enhance the prognostic performance further **(10 points max)**.\n",
    "\n",
    "N.B. The maximum of points can not exceed 100 points together with the bonus task.\n",
    "\n",
    "## II. Report Submission:\n",
    "\n",
    "Upon completion of the implementation and evaluation tasks, you are required to submit a comprehensive report summarizing your work and findings. Your report should span 5-8 pages and should be structured as follows:\n",
    "\n",
    "#### Introduction:\n",
    "- A brief introduction/literature review of 1D Convolutional Neural Networks (1DCNN) and their application in time series data analysis, particularly in prognostics.\n",
    "\n",
    "#### Model Description:\n",
    "- A detailed description of the 1DCNN model you implemented, including the architecture, layers, and any regularization techniques used.\n",
    "- Mention of the hyperparameters that were tuned, and any additional modifications made to the original model architecture from the paper.\n",
    "\n",
    "#### Methodology:\n",
    "- Description of the data preparation process, training, and evaluation methodologies.\n",
    "- Mention the training and testing units used as specified in the task.\n",
    "\n",
    "#### Results and Discussion:\n",
    "- Summarization of the evaluation results across 5 runs, providing the mean and standard deviation for both RMSE and NASA's Scoring Function.\n",
    "- Evaluation of the performance per unit, and comparison of your model's performance with the results reported in the referenced paper.\n",
    "- Discussion on the impact of Dropout and Batch Normalization (if applied), and any other observations regarding the model's performance.\n",
    "- Any challenges faced during the implementation and how they were addressed.\n",
    "\n",
    "#### Conclusion:\n",
    "- Summary of key findings, lessons learned, and suggestions for future work to possibly improve the model's performance.\n",
    "\n",
    "#### References:\n",
    "- Proper citation of the paper, any other related works, and resources utilized in completing the assignment.\n",
    "\n",
    "Ensure your report is well-organized, clear, and concise. Visual aids like graphs, tables, and diagrams should be used to enhance the explanation of your work, and comparisons made. The discussion should provide insightful analysis on the model's performance and a critical comparison with the paper's results. The report should be submitted in PDF format by **6.11.2024 Midnight 23:59**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "24ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
