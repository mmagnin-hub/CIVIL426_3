{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Introduction to Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNN) are a cornerstone of deep learning, specialized in analyzing spatial or temporal data. \n",
    "The core of CNN lies in its convolution layers—conv1D, conv2D, and conv3D—which autonomously extract hierarchal features from the input data, making CNNs highly efficient and interpretable. \n",
    "Understanding the mechanics of these convolution operations is important. In this tutorial, we will delve into the 1D, 2D, and 3D convolutions and explore their implementation in PyTorch, providing a hands-on understanding of how to leverage CNNs for various data formates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/conv_overview.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)```\n",
    "\n",
    "<img src=\"images/1dcnn_overview.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight learnt in 1DCNN\n",
    "\n",
    "the convolution kernel learned in 1DCNN has weight and bias. The shape of weight is (out channel, in channel / group, kernel size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1dcnn_weight.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1dcnn_convolution.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stride\n",
    "refers to the step size that the convolutional kernel takes as it slides across the input data. \n",
    "It is often used to reduce the spatial dimention (and thus also the computational efficiency) and increase receptive field.\n",
    "The following figure shows different convolution bands, where red represents the first convolution and purple represents the second convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1dcnn_stride.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dilation\n",
    "spacing between the values in the kernel. \n",
    "A dilation of 1 means no spacing between the kernel values, which is the standard convolution. \n",
    "A dilation of 2 means one value spacing between kernel values, and so on.\n",
    "\n",
    "Dilation is used to expand the receptive field of the kernel without increasing the number of parameters in the kernel (compare to stride)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1dcnn_dilation.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D & 3D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2D and Conv3D are extensions of the convolution operation to 2 and 3 dimensions. They operate under the same principle as Conv1D, but at higher dimensions.\n",
    "\n",
    "Even when handling multi-channel images (e.g., RGB images), Conv2D operates in a 2D plane for each channel, without intertwining information across channels in the depth dimension.\n",
    "Unlike Conv2D, Conv3D has the ability to convolute in the depth direction, thus able to capture spatial-temporal features in scenarios like video processing where frames add a third dimension.\n",
    "\n",
    "The distinction in handling the depth dimension is crucial. In Conv2D, each channel is treated separately on a 2D plane, whereas Conv3D handles the depth dimension as an integral part of the convolution operation, allowing for multi-channel convolution along the depth. Hence, the term \"depth\" in Conv3D doesn't correspond to the \"channel\" concept in Conv2D. Each depth slice in Conv3D can contain multiple channels, establishing a distinction between depth and channel, and enabling richer feature extraction from multi-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/xdcnn_kernel.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D (Simple Crack Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function using np.dot()\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "# load crack image from the images folder\n",
    "img = plt.imread('images/crack.png')\n",
    "print(img.shape)\n",
    "# convert to grayscale\n",
    "img = rgb2gray(img)\n",
    "print(img.shape)\n",
    "# visualize the image\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Crack Image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the image to a tensor\n",
    "crack_tensor = torch.tensor(img)\n",
    "crack_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a (2x2) kernel to detect vertical edges\n",
    "v_kernel = torch.tensor([[-1., 1.], [-1., 1.]])\n",
    "# apply the kernel to the image\n",
    "# input shape: (b, #in_chanels, height, width)\n",
    "# weight shape: (#out_chanels, #in_chanels, kernel_height, kernel_width)\n",
    "crack_tensor = crack_tensor.unsqueeze(0).unsqueeze(0)\n",
    "v_edges = nn.functional.conv2d(input=..., weight=..., padding='same')\n",
    "v_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize v_edges\n",
    "plt.imshow(..., cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a (2x2) kernel to detect horizonal edges\n",
    "h_kernel = torch.tensor([[..., ...], [..., ...]])\n",
    "# apply the kernel to the image\n",
    "# input shape: (b, #in_chanels, height, width)\n",
    "# weight shape: (#out_chanels, #in_chanels, kernel_height, kernel_width)\n",
    "h_edges = nn.functional.conv2d(input=..., weight=..., padding='same')\n",
    "h_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize h_edges\n",
    "plt.imshow(..., cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D (simple human activity recognition)\n",
    "\n",
    "Can you detect Sitting, Walking, Running Phases from the given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example time series for acceleration in x, y, z directions over a longer time\n",
    "time_steps = np.linspace(0, 10, 100)  # 100 time steps\n",
    "\n",
    "x1 = np.random.normal(0.1, 0.02, 30)  \n",
    "y1 = np.random.normal(0.05, 0.01, 30)\n",
    "z1 = np.random.normal(0.9, 0.02, 30)\n",
    "\n",
    "x2 = np.random.normal(1.0, 0.3, 30)    \n",
    "y2 = np.random.normal(0.7, 0.2, 30)\n",
    "z2 = np.random.normal(1.1, 0.2, 30)\n",
    "\n",
    "x3 = np.random.normal(2.0, 0.5, 40)\n",
    "y3 = np.random.normal(1.5, 0.4, 40)\n",
    "z3 = np.random.normal(2.2, 0.4, 40)\n",
    "\n",
    "# Concatenate to create a full time series\n",
    "x_data = np.concatenate([x1, x2, x3])\n",
    "y_data = np.concatenate([y1, y2, y3])\n",
    "z_data = np.concatenate([z1, z2, z3])\n",
    "\n",
    "# Plot the acceleration data for each axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_steps, x_data, label=\"x-axis\", color='r')\n",
    "plt.plot(time_steps, y_data, label=\"y-axis\", color='g')\n",
    "plt.plot(time_steps, z_data, label=\"z-axis\", color='b')\n",
    "plt.title(\"Acceleration in x, y, z directions\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Acceleration\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data, shape: (b, #in_chanels, length)\n",
    "input_data = torch.tensor([x_data, y_data, z_data], dtype=torch.float32).view(...)\n",
    "\n",
    "# kernel of size 1 to detect sum of acceleration in x, y, z directions, shape: (#out_chanels, #in_chanels, kernel_size)\n",
    "sum_kernel = torch.tensor([...]).view(...)\n",
    "# apply the kernel to the image\n",
    "# \n",
    "sum_acceleration = nn.functional.conv1d(input=input_data, weight=sum_kernel)\n",
    "sum_acceleration.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sum of acceleration\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_steps, sum_acceleration[0, 0].detach().numpy(), label=\"sum of acceleration\", color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "24ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
